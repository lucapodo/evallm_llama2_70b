# EvaLLM Case Study

Welcome to the EvaLLM Case Study repository! This repository provides insights and data from our preliminary study on the evaluation of Llama2-70B using EvaLLM.

## Contents

- [Ongoing Work](#ongoing-work)
- [Data](#data)
- [Code Implementation](#code-implementation)
- [How to Contribute](#how-to-contribute)
- [Contact](#contact)

## Ongoing Work

Our ongoing efforts include:

1. Running the stack on the entire nBench dataset.
2. Comparing multiple datasets to analyze and understand performance variances.
3. Developing an error taxonomy to categorize and understand the types of errors made by LLMs.

## Data

The data used in our experiments can be found in:

- `llama-70b-results-v5.csv`: Contains the results and data from our LLM evaluations.

## Code Implementation

The code implementation for this case study is maintained in a separate repository. You can find all the code, including scripts for data processing, model evaluation, and analysis, in the following repository:

[Code Implementation Repository](https://github.com/ishmalazmi/llm4vis-benchmark/)

## How to Contribute

We welcome contributions to this project! If you find our work interesting or useful, please consider:

- Starring this repository to show your support.
- Watching the repository to stay updated with our progress.
- Contributing by opening issues or pull requests.

## Contact

If you have any questions, suggestions, or would like to collaborate, please feel free to contact us:

- **Email:** [podo@di.uniroma1.it](mailto:podo@di.uniroma1.it)
- **Linkedin:** [lucapodo](https://www.linkedin.com/in/luca-podo/)

Thank you for your interest in the EvaLLM Case Study!
